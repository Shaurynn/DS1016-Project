{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numbers import Number\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_example(img: str, label: Number) -> tf.train.Example:\n",
    "    # convert to numpy arraw\n",
    "    nii0 = np.asanyarray(nib.load(img).dataobj)\n",
    "    \n",
    "    # rescale to 0-1\n",
    "    # if you want to use tf.image.per_image_standardization(), this would be the place to do so\n",
    "    # instead of the rescaling\n",
    "    nii = (nii0 - nii0.min())/(nii0.max() - nii0.min()).astype(np.float32) \n",
    "\n",
    "    feature = {\n",
    "        'label': _float_feature(label),\n",
    "        'image_raw': _bytes_feature(tf.io.serialize_tensor(nii).numpy())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_records(niis, labels, n_per_record: int, outfile: str) -> None:\n",
    "    \"\"\"\n",
    "    store list of niftis (and associated label) into tfrecords for use as dataset\n",
    "    \"\"\"\n",
    "    n_niis = len(niis)\n",
    "    n_records = math.ceil(len(niis) / n_per_record)\n",
    "\n",
    "    for i, shard in enumerate(range(0, n_niis, n_per_record)):\n",
    "        print(f\"writing record {i} of {n_records-1}\")\n",
    "        with tf.io.TFRecordWriter(\n",
    "                f\"{outfile}_{i:0>3}-of-{n_records-1:0>3}.tfrecords\", \n",
    "            options= tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "        ) as writer:\n",
    "            for nii, label in zip(niis[shard:shard+n_per_record], labels[shard:shard+n_per_record]):\n",
    "                example = serialize_example(img=nii, label=label)\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def parse_1_example(example) -> tf.Tensor:\n",
    "    X = tf.io.parse_tensor(example['image_raw'], out_type=tf.float32)\n",
    "    return tf.expand_dims(X, 3), example['label'] \n",
    "\n",
    "\n",
    "def decode_example(record_bytes)-> dict:\n",
    "    example = tf.io.parse_example(\n",
    "        record_bytes,     \n",
    "        features = {\n",
    "          \"label\": tf.io.FixedLenFeature([], dtype=tf.float32),\n",
    "          'image_raw': tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "          }\n",
    "    )\n",
    "    return example\n",
    "\n",
    "def get_batched_dataset(files, batch_size: int = 32, shuffle_size: int=1024) -> tf.data.Dataset:\n",
    "    dataset = (\n",
    "        tf.data.Dataset.list_files(files) # note shuffling is on by default\n",
    "        .flat_map(lambda x: tf.data.TFRecordDataset(x, compression_type=\"GZIP\", num_parallel_reads=8))\n",
    "        .map(decode_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(parse_1_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .cache()  # remove if all examples don't fit in memory (note interaction with shuffling of files, above)\n",
    "        .shuffle(shuffle_size)\n",
    "        .batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# store example set of image with labels 0-9\n",
    "# (e.g., put nifti of label MNI152_T1_1mm_brain.nii.gz in the working directory)\n",
    "mni_nii = ['../raw_data/MNI152_T1_1mm.nii.gz']*10\n",
    "\n",
    "# store examples in each tfrecord. number of examples per record is configurable.\n",
    "# aim for as many examples as produces files of size > 100M \n",
    "write_records(mni_nii, [x for x in range(10)], 10, \"tmp\")\n",
    "\n",
    "# read the records back. this will be the list of files generated by write_records()\n",
    "# a full dataset will have a list with many records\n",
    "list_of_records=['tmp_000-of-000.tfrecords']\n",
    "ds = get_batched_dataset(list_of_records, batch_size=2, shuffle_size=10)\n",
    "\n",
    "# ds can now be passed to model.fit\n",
    "# but first!!!!! \n",
    "# the serialization is a lot, so it is a good idea to verify that the images\n",
    "# look okay when loaded\n",
    "(Xs, Ys) = next(ds.as_numpy_iterator())\n",
    "\n",
    "# (batch_size, )\n",
    "# order will depend on shuffle (turn off all shuffling to verify order)\n",
    "Ys.shape\n",
    "\n",
    "# (batch_size, x_dim, y_dim, z_dim, 1)\n",
    "Xs.shape\n",
    "\n",
    "# convert first element in batch to in-memory nibabel.nifti format for display\n",
    "nii = nib.Nifti1Image(np.squeeze(Xs[0,]), affine=np.eye(4)*2)\n",
    "\n",
    "# this should look like a typical brain\n",
    "plotting.plot_anat(nii)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfccd179c622ea5fc33f44bb94e37523dc7d9ad856bc3d150a4186024dd1b1ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
